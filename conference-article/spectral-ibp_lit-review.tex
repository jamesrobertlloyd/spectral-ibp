\documentclass{article}
% Style file
\usepackage{assets/nips12submit_e,times}
% Basic packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb, bm}
\usepackage{verbatim}
% More fun stuff
\usepackage{graphicx}%\graphicspath{{figures/}}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage[square,numbers]{natbib}
%\usepackage[usenames]{color}
\usepackage{mathrsfs}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,linkcolor=blue]{hyperref}
\usepackage{hypernat}
\usepackage{datetime}
\usepackage{textcomp}
\usepackage{assets/picins}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes,calc}
\tikzstyle{mybox} = [draw=white, rectangle]
\usepackage{booktabs}

% Algorithms, and some standard modifications
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\algrule}[1][.2pt]{\par\vskip.3\baselineskip\hrule height #1\par\vskip.3\baselineskip}

\title{
A spectral approximation to the Indian Buffet Process
}

\author{
J Lloyd, Z Ghahramani, C Reed\\
Department of Engineering\\
Cambridge University\\
%\texttt{jrl44@cam.ac.uk, FIX ME}
%\And
%Coauthor \\
%Affiliation \\
%Address \\
%\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\setlength{\marginparwidth}{1.25in}

\input{assets/commenting.tex}

%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  %% Note, NA's pass through!

\input{assets/defs.tex}

\nipsfinalcopy % Uncomment for camera-ready version

\numberwithin{equation}{section}
\numberwithin{thm}{section}

% Document specific notation

\DeclareMathOperator*{\argmin}{\arg\!\min}

\begin{document}

\maketitle

\begin{abstract}
Literature review of spectral clustering for maximum-likelihood IBP formulation.
\end{abstract}

\section{Paper Summaries}
\subsection{Fast Approximate Spectral Clustering: \citet{yan2009fast}}
\citet{yan2009fast} discuss fast approximate spectral clustering framework based on minimizing the distortion (misclustering) of the resulting clusters as compared to non-approximate spectral clustering. Specifically, they introduce a K-means and RP-trees preprocessing step that forms a set of representative points and then performs spectral clustering on the representative points. These algorithms are referred to as \textit{KASP} and \textit{RASP}, respectively. The complexity of KASP is $O(knt) + O(k^3)$ while RASP is $O(hn) + O(k^3)$, where $k$ is the number of representative points, $n$ is the number of data points, $t$ is the number of iterations of the preprocessing algorithms, and $h$ is the depth of the RP tree.

\citet{yan2009fast} use a perturbation analysis of the graph Laplacian to show that the misclustering of (K|R)ASP converges to 0 as the number of representative points increases. Furthermore, they show that their techniques perform much better than K-means and comparable to a Nystr\"om approximation but with smaller memory requirements and runtime.


\subsection{Non-redundant Multi-view Clustering via Orthogonalization: \citet{Cui2007}}
\citet{Cui2007} propose general methods for forming multiple clustering solutions for a given dataset, where each clustering is in a different subspace. Their method operates iteratively by clustering the data using k-means, orthogonalizing the data to a new subspace that is not covered by the previous clusters, clustering the data in the new subspace, et cetera until reaching some form of convergence.  The authors discuss two approaches to performing this clustering: orthogonal clustering and clustering in orthogonal spaces.

\textit{Orthogonal clustering}: given current data $X^{(t)}$ and the clustering solution of $X^{(t)}$, $M^{(t)} = [\mu_1^{(t)} \mu_2^{(2)} \cdots \mu_k^{(t)}]$, \citet{Cui2007} view orthogonal clustering as projecting the data points from the original data space to a compressed space (subspace) spanned by the mean vectors. They describe two variations for performing clustering in the space that is orthogonal to the compressed data space based on the hard and soft clustering views:
\begin{itemize}
\item hard clustering: each data point $x_i^{(t)}$ belonging to cluster $j$ is projected onto the cluster center $\mu_j^{(t)}$ and $x_i^{(t+1)}$ is determined as $x_i^{(t)}$ projected onto the subspace orthogonal to the cluster centroids:
$$
x_i^{(t+1)} = (I - \mu_j^{(t)}\mu_j^{(t)T}/\mu_j^{(t)T}\mu_j^{(t)})x_i^{(t)}
$$
%
\item soft clustering: since each data point can partially belong to each cluster, we can project $X^{(t)}$ onto all cluster means and compute $X^{(t+1)}$ as $X^{(t)}$ projected onto the subspace orthogonal to all the cluster centroids:
$$
X^{(t+1)} = (I-M^{(t)}(M^{(t)T}M^{(t)})^{-1}M^{(t)T}X^{(t)} 
$$
\end{itemize}
%

\textit{Clustering in orthogonal subspaces}: \citet{Cui2007} view clustering in orthogonal spaces as projecting the data into a reduced dimensional space that discriminates the given classes: i.e.\ via latent discriminant analysis or PCA. In particular, given current data $X^{(t)}$ and the clustering solution of $X^{(t)}$, $M^{(t)} = [\mu_1^{(t)} \mu_2^{(2)} \cdots \mu_k^{(t)}]$, the authors find the PCA solution of $M^{(t)}$, keep the $k^{(t)}-1$ eigenvectors to obtain the subspace $A^{(t)}$ that captures the current clustering, and then projects $X^{(t)}$ to the space orthogonal to $A^{(t)}$:
$$
X^{(t+1)} = (I-A^{(t)}(A^{(t)T}A^{(t)})^{-1}A^{(t)T}X^{(t)} 
$$

\textit{Key observations}: \citet{Cui2007} used k-means for clustering and is therefore limited to convex clusters.v

\subsection{Multiple Non-Redundant Spectral Clustering Views: \citet{Niu2010}}
Notation summary
\begin{itemize}
\item $G$: $G=\{V,E\}$ is the graph used to define the $NCut$ formulation of spectral clustering
\item $K$: similarity matrix with elements $k_{ij}$ measure the similarity between vertices $i$ and $j$ in $G$
\item $P_i$: partition/cluster $i$
\item $d_i$: degree of vertex $i$ with $d_i = \sum_{j=1}^n k_{ij}$
\item $U$: indicator matrix: assigns vertex (row) to cluster (c) --- takes real values in relaxed spectral clustering formalism
\item $n$: number of elements
\item $c$: number of clusters
\item $W_q$: $\mathbb{R}^{d\times l_q}$ transformation matrix for each view that transforms the data in the original space to the lower-dimensional space
\item $m$: number of subspaces (number of multiple clustering solutions)
\item $q$: indexes the subspaces
\item $x$: the input data
\end{itemize}

The central idea of this work was to determine multiple non-redundant spectral clustering views by learning non-redundant subspaces and performing spectral clustering in these subspaces. Specifically, this was accomplished by using the NCut spectral clustering formulation over a similarity graph $G=\{V,E\}$ with disjoint partitions $P_1, \ldots, P_c$, and similarity matrix $K$:
$$
NCut(P_1, \ldots, P_c) = \sum_{t=1}^c \frac{cut(P_t, V\\P_t}{vol(P_t)},
$$
where
$$
cut(\mathcal{A}, \mathcal{B}) = \sum_{v_i \in \mathcal{A}, v_j \in \mathcal{B}} k_{ij},
$$
and degree $d_i = \sum_{j=1}^n k_{ij}$ so that $vol(\mathcal{A}) = \sum_{i \in \mathcal{A}} d_i$.
By introducing the indicator matrix $U$ and allowing this matrix to take on values in $[0,1]$ (a relaxation of the original discrete optimization problem), cluster assignment optimization reduces to the well-known trace-maximization problem:
\begin{align*}
& \text{max}_{U \in \mathbb{R}^{n \times c}} ~\text{tr}(U^TD^{-1/2}KD^{-1/2}U)\\
& \text{s.t. } U^TU = 1
\end{align*}

\citet{Niu2010} use the kernel similarity matrix $K_q$ computed with the kernel function $k(W^T_q x_i, W^T_q x_j)$ where $W_q \in \mathbb{R}^{d\times l_q}$ transforms the original data to subspace $q$, where $q \in \{1, \ldots, m\}$. The trick used by the authors is to introduce a penalty (regularization term) for redundant kernel spaces via the Hilbert-Schmidt Independence Criterion, so that our optimization problem becomes:
\begin{align*}
& \text{max}_{U_1, \ldots, U_m, W_1, \ldots, W_m}  ~\text{tr}(U_q^TD_q^{-1/2}K_qD_q^{-1/2}U_q) - \lambda \sum_{q \neq r} \text{HSIC}(W_q^Tx, W_r^Tx)\\
& \text{s.t. } U_q^TU_q = 1 \\
& (K_q)_{ij} = k_q(W_q^Tx_i,W_q^Tx_j) \\ 
& W_q^TW_q = I
\end{align*}
which is optimized in two steps:
\begin{enumerate}
\item Assume all $W_q$ fixed, optimize $U_q$ in each view (perform spectral clustering in the given subspace---simple)
\item Assume all $U_q$ fixed, optimize $W_q$ in each view ---- used gradient ascent on the Stiefel manifold\footnote{Colorado does not understand this optimization technique}
\end{enumerate}

Finally, initialization is important in this algorithm. The authors initialize the $W_q$ matrices by performing spectral clustering with a similarity matrix $A$ that measures similarity between the original features of the dataset using HSIC and then clusters the features into $m$ clusters ($m$ is a parameter). Then for each feature assigned to cluster $q$, append a $d \times 1$ column to $W_q$ with all entries 0 except the $j^{th}$ element which is set to 1. This effectively creates $m$ subspaces where each subspace clusters on a disjoint subset of the features. However, the final learned $W_q$ can have features with weights in many views.




\section{Literature to Investigate}
\begin{itemize}
\item \citet{Griffiths2011}
\item \citet{Luxburg2007}
\item \citet{Cui2007}
\item \citet{Niu2010}
\item \citet{Niu2011}
\item \citet{Niu2012}
\item \citet{Higham2004}
\item \citet{yan2009fast}
\item \citet{jain2008simultaneous}
\end{itemize}
% Bibliography

%\newpage
\small{
\bibliographystyle{assets/natbib}
\bibliography{library}
}

\end{document}
