\documentclass{article}
% Style file
\usepackage{assets/nips12submit_e,times}
% Basic packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb, bm}
\usepackage{verbatim}
% More fun stuff
\usepackage{graphicx}%\graphicspath{{figures/}}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage[square,numbers]{natbib}
%\usepackage[usenames]{color}
\usepackage{mathrsfs}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,linkcolor=blue]{hyperref}
\usepackage{hypernat}
\usepackage{datetime}
\usepackage{textcomp}
\usepackage{assets/picins}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes,calc}
\tikzstyle{mybox} = [draw=white, rectangle]
\usepackage{booktabs}

% Algorithms, and some standard modifications
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\algrule}[1][.2pt]{\par\vskip.3\baselineskip\hrule height #1\par\vskip.3\baselineskip}

\title{
A spectral approximation to the Indian Buffet Process
}

\author{
James Robert Lloyd, Zoubin Ghahramani\\
Department of Engineering\\
Cambridge University\\
%\texttt{jrl44@cam.ac.uk, FIX ME}
%\And
%Coauthor \\
%Affiliation \\
%Address \\
%\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\setlength{\marginparwidth}{1.25in}

\input{assets/commenting.tex}

%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  %% Note, NA's pass through!

\input{assets/defs.tex}

\nipsfinalcopy % Uncomment for camera-ready version

\numberwithin{equation}{section}
\numberwithin{thm}{section}

% Document specific notation

\def\IBP{Z}
\def\Weights{A}
\def\Data{X}
\def\Graph{G}
\def\Adjacency{W}
\def\Degree{D}
\def\Laplacian{L}

\DeclareMathOperator*{\argmin}{\arg\!\min}

\begin{document}

\maketitle

\begin{abstract}
Can we use spectral methods to get a fast model based on the IBP?
Or maybe approximation schemes developed for spectral clustering can be carried over to the IBP case?
\end{abstract}

\section{An IBP model}

Consider a simple linear Gaussian model of the form
\begin{eqnarray}
\Weights & \dist & \Normal\,(0, \sigma_\Weights^2 I) \\
\IBP & \dist & \textrm{IBP} \\
\Data & \dist & \Normal\,(\IBP \Weights, \sigma_\Data^2 I).
\end{eqnarray}
Copying from \cite{Griffiths2011} we should be guided by terms such as
\begin{eqnarray}
p\,(\Data | \IBP, \Weights) & \propto & \exp\,(-\textrm{tr}((\Data - \IBP\Weights)\Transpose(\Data - \IBP\Weights))) \\
p\,(\Data | \IBP) & \propto & |\IBP\Transpose\IBP + \frac{\sigma_\Data^2}{\sigma_\Weights^2}I|^{D/2} \exp\,(-\textrm{tr}(\Data\Transpose(I - \IBP(\IBP\Transpose\IBP + \frac{\sigma_\Data^2}{\sigma_\Weights^2}I)^{-1}\IBP\Transpose)\Data)).
\end{eqnarray}
For simplicity, we might initially consider simpler priors on $\IBP$ and or a maximum likelihood framework.
\section{Spectral clustering notes}

For a similarity matrix $\Graph$, with adjacency matrix $\Adjacency$ and degree diagonal matrix $\Degree$, the unnormalized graph Laplacian is defined as
\begin{equation}
\Laplacian = \Degree - \Weights
\end{equation}
has the following property \citep{Luxburg2007}
\begin{eqnarray}
f' \Laplacian f = \frac{1}{2}\sum w_{ij}(f_i - f_j)^2
\end{eqnarray}
and the multiplicity of the zero eigenvalue of $\Laplacian$ is equal to the number of connected components in $\Graph$.

Spectral clustering is approximately solving the min cut problem for the weighted adjacency matrix \ie
\begin{equation}
\argmin_{\IBP_1} = W(\IBP_1, \bar{\IBP_1}) := \sum_{i \in \IBP_1, j \in \bar{\IBP_1}}w_{ij}
\end{equation}

\section{Relations between the two}

Suppose we are trying to estimate $\Weights$ and $\IBP$ by maximum likelihood.
In particular, consider estimating the $k$th column of $\IBP$ and the corresponding row of $\Weights$ keeping all other parameters fixed.
Our objective can be stated as trying to minimise
\begin{equation}
||\Data - \IBP_{-k}\Weights_{-k} - \IBP_{k}\Weights_{k}||
\end{equation}
where $||.||$ is some distance metric on matrices (\ie the appropriate one to make this equivalent to maximum likelihood).

\fTBD{I have implicitly assumed that $z_{ij} \in \{-1,1\}$ so that we can model bias terms etc A minor point but might be useful for some forms of identifiability}
Let $\tilde X = \Data - \IBP_{-k}\Weights_{-k}$.
For a given $\IBP_k$, the maximum likelihood estimation of $\Weights$ is equivalent to minimising
\begin{equation}
\sum_{i \in \IBP_k}|\tilde x_i - \beta_k| + \sum_{i \in \bar\IBP_k}|\tilde x_i - \bar\beta_k|
\end{equation}
over $\beta_k$ and $\bar\beta_k$.
We can now create a link to spectral clustering.

Consider the following constant
\begin{eqnarray}
C & = & \sum_{i,j}|\tilde x_i - \tilde x_j| \\
  & = & \sum_{i,j \in \IBP_k}|\tilde x_i - \tilde x_j| + \sum_{i,j \in \bar\IBP_k}|\tilde x_i - \tilde x_j| + 2\sum_{i \in \IBP_k, j \in \bar\IBP_k}|\tilde x_i - \tilde x_j|
\end{eqnarray}
and then consider maximising $\sum_{i \in \IBP_1, j \in \bar\IBP_1}|\tilde x_i - \tilde x_j|$ over $\IBP_k$.
This can be recast as minimising quantities of the form $\sum_{i \in \IBP_k, j \in \bar\IBP_k}(1 - \alpha|\tilde x_i - \tilde x_j|)$ for any $\alpha > 0$.
For small enough $\alpha$ all summands will be positive and this can be phrased as a min cut problem \ie approximate spectral clustering.

Thus, spectral clustering is approximately equivalent to minimising $\sum_{i,j \in \IBP_k}|\tilde x_i - \tilde x_j| + \sum_{i,j \in \bar\IBP_k}|\tilde x_i - \tilde x_j|$.

Let $\hat\beta_k = \argmin_\beta \sum_{i \in \IBP_k}|\tilde x_i - \beta|$. Using this definition and the triangle inequality, we get the following
\begin{equation}
|\IBP_k|\sum_{i \in \IBP_1}|\tilde x_i - \hat\beta| \leq \sum_{i,j \in \IBP_1}|\tilde x_i - \tilde x_j| \leq 2|\IBP_k|\sum_{i \in \IBP_1}|\tilde x_i - \hat\beta_k|.
\end{equation}
\ie we can show that min cut is optimising a bound on the maximum likelihood objective.

The bound is only tight when the problem is degenerate \ie this is not yet a guarantee, just a heuristic.
The tightness of these bounds could be demonstrated in a probabilistic sense by assuming the data was generated by a linear binary model.

\section{A natural iterative algorithm}

In the maximum likelihood setting this is easy, the following algorithm can be justified using the arguments above.
\begin{itemize}
\item Find a binary clustering, $\IBP_1$, using spectral clustering applied to \Data
\item Fit maximum likelihood parameters to yield $\hat\Weights_1$
\item Obtain a new clustering, $\IBP_2$, by applying spectral clustering to $\Data - \IBP_1\Weights_1$. This is a sort of iterative conditional maximisation algorithm
\item Fit maximum likelihood parameters to yield $\hat\Weights_{1:2}$
\item Obtain a new clustering, $\IBP_{3}$, by applying spectral clustering to $\Data - \IBP_{1:2}\Weights_{1:2}$. This is a sort of iterative conditional maximisation algorithm
\item et cetera\ldots with looping, split merge equivalents and other nice things developed for samplers.
\end{itemize}

\section{Literature review}

\TBD{Write notes here}

\cite{Griffiths2011}
\cite{Luxburg2007}
\cite{Cui2007}
\cite{Niu2010}
\cite{Niu2011}
\cite{Niu2012}

\section{$k$-means clustering as a generic preprocessing step}

I am currently using the approximate spectral clustering algorithm of \cite{Yan2009}.
Is this a good idea?
I will try to work out what is going on in this approximation.

\section{Next steps / questions}
\begin{itemize}
\item Can we modify the above argument to be more Bayesian?
\item In doing so, can we look into the min cut / spectral clustering approximation to find an appropriate graph Laplacian?
\item How different is this to orthogonal projections \citep{Cui2007}?
\item Try it? Choosing sensible parameters for the spectral clustering will not be entirely easy.
\item Can I make a guarantee about improving the marginal likelihood? Possibly not?
\end{itemize}

\section{Some relevant literature}

% Bibliography

%\newpage
\small{
\bibliographystyle{assets/natbib}
%\bibliographystyle{icml2013}
\bibliography{library}
}

\end{document}
