\documentclass{article}
% Style file
\usepackage{assets/nips12submit_e,times}
% Basic packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb, bm}
\usepackage{verbatim}
% More fun stuff
\usepackage{graphicx}%\graphicspath{{figures/}}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage[square,numbers]{natbib}
%\usepackage[usenames]{color}
\usepackage{mathrsfs}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,linkcolor=blue]{hyperref}
\usepackage{hypernat}
\usepackage{datetime}
\usepackage{textcomp}
\usepackage{assets/picins}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes,calc}
\tikzstyle{mybox} = [draw=white, rectangle]
\usepackage{booktabs}

% Algorithms, and some standard modifications
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\algrule}[1][.2pt]{\par\vskip.3\baselineskip\hrule height #1\par\vskip.3\baselineskip}

\title{
$k$-means, Infinite Latent Feature Models\\ and Multiple Clustering
}

\author{
James Robert Lloyd, Colorado Reed, Zoubin Ghahramani\\
Department of Engineering\\
Cambridge University\\
%\texttt{jrl44@cam.ac.uk, FIX ME}
%\And
%Coauthor \\
%Affiliation \\
%Address \\
%\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\setlength{\marginparwidth}{1.25in}

\input{assets/commenting.tex}

%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  %% Note, NA's pass through!

\input{assets/defs.tex}

\nipsfinalcopy % Uncomment for camera-ready version

\numberwithin{equation}{section}
\numberwithin{thm}{section}

% Document specific notation

\def\IBP{Z}
\def\Weights{A}
\def\weights{a}
\def\Data{X}
\def\data{x}
\def\Residuals{\tilde{X}}
\def\residuals{\tilde{x}}
\def\Graph{G}
\def\Adjacency{W}
\def\Degree{D}
\def\Laplacian{L}
\def\IBPPrior{\alpha}
\def\errors{\epsilon}
\def\bias{b}
\def\centre{\mu}

\DeclareMathOperator*{\argmin}{\arg\!\min}

\begin{document}

\maketitle

\begin{abstract}
PLACEHOLDER

We demonstrate that $k$-means based multiple clustering models can be viewed as MAP inference in probabilistic models.
The probabilistic model is an approximation to a nonparametric model, and better $k$-means algorithms are developed by approximating more advanced models.
This also extends recent work on links between infinite Gaussian mixtures and $k$-means by producing a multi-view clustering algorithm.
\end{abstract}

\section{Introduction}

\PROBLEM{Placeholder}

Traditional clusterings \citep[e.g.][]{Jain2000} produce one solution.
However, data is usually more interesting and multi-faceted.

The Indian Buffet Process (IBP) is a distribution over infinite binary matrices \citep[e.g.][]{Griffiths2011}.
The IBP is often used as a prior that assigns a latent binary sequence to data points which can be interpreted as membership in multiple binary clusterings of the data.
We show that the maximum likelihood solution to a parametric version of this model is equivalent to a $k$-means algorithm for multiple clustering already in the literature \citep{Cui2007}.
By drawing a parallel with the probabilistic model and considering MAP inference we construct a regularised \citep[e.g.][]{Sun2012} and `nonparametric' modification to the $k$-means algorithm that retains efficiency.

We then extend this approximation approach to a more sophisticated Bayesian nonparametric model for multiple clustering \citep{Niu2012}.
This model automatically performs feature selection for each view, selects the number of views and the number of clusters in each view.
We again show how a limiting form of this algorithm can be approximately solved using $k$-means.
In particular, we combine feature selection methodologies of \citep{Dy2004}, with the `nonparametric' $k$-means algorithm of \cite{Kulis2012} to produce an efficient approximation to the model of \cite{Niu2012}.
\TBD{In this model, sharing of features happens through IBP `stickiness' in the two parameter version.
We could also try an additive version.
Which would be better?}

We hope that our two approximations will show how richly structured non-parametric models can in general be approximated.
One should probably compare to \eg variational inference in these models.
The scaling could potentially be a lot better though.

\section{Related work}

\TBD{To do}

\section{Simple clustering models}

\subsection{Gaussian mixture models and $k$-means}

It is well known that there is a correspondence between Gaussian mixture models and the $k$-means algorithm.
Pr\'ecising the exposition in \cite{Kulis2012}, a Gaussian mixture model assumes that data arises from the following distribution
\begin{equation}
p(x) = \sum_{c=1}^k \pi_c \, \Normal(x \given \mu_c, \Sigma_c).
\end{equation}
The $k$-means algorithm attempts to optimise the following objective
\begin{equation}
\min_{(\ell_c)} \sum_{c=1}^k \sum_{x \in \ell_c} ||x - \mu_c||^2 \quad \textrm{where} \quad \mu_c = \frac{1}{|\ell_c|}\sum_{x \in \ell_c}x.
\end{equation}
It can be shown that when $\Sigma_c = \sigma I$ and as $\sigma \to 0$, an EM algorithm for the Gaussian mixture model tends towards the $k$-means algorithm, establishing the correspondence.

\subsection{Binary linear Gaussian models and $k$-means}

We now show a correspondence between a simple probabilistic model for multiple clustering and $k$-means.
Consider the following model for an array of data
\begin{equation}
\Data_{ij} \sim \Normal((\IBP\Weights)_{ij} + \bias_{j}, \sigma_\Data)
\end{equation}
where $\IBP$ is a binary matrix, $\Weights$ is real valued weight matrix and $\bias$ is a bias vector. This is a variant of a model commonly used to demonstrate the Indian Buffet Process \citep[e.g.][]{Griffiths2011}. The log posterior in this model can be written as
\begin{equation}
\log(p(\IBP)) + \log(p(\Weights)) + \log(p(\bias)) - \sum_{ij}(\Data_{ij} - (\IBP\Weights)_{ij} - \bias_j)^2.
\end{equation}
Assuming uniform priors on the log scale\fTBD{\ie scale free priors for the weights and bias and a uniform prior on $\IBP$}, MAP inference is equivalent to minimising the following sum of squares
\begin{equation}
\sum_{i: \IBP_{ik} = 1}||\Residuals^k_{i:} - \centre^k||^2 + \sum_{i: \IBP_{ik} = 0}||\Residuals^k_{i:} - \bar{\centre}^k||^2
\end{equation}
over $\centre^k$ and $\bar{\centre}^k$ where we have defined the residuals matrix $\Residuals^k_{ij} = \Data_{ij} - \sum_{m \neq k}\IBP_{im}\Weights_{mj}$.
The minimising $\centre$ are simply given by appropriate averages.
Thus, when jointly optimising over $\IBP_{:k}$ and $\centre$ we recover the $2$-means objective.

Therefore a reasonable heuristic for MAP inference in this model is to iteratively perform $2$-means clustering on the matrix of residuals.

\subsection{Beyond binary clustering}

The Binary linear Gaussian model above is similar to a Gaussian mixture model, but one of the main differences is that the means of the different clusters have an additive structure \ie $\centre_c = \sum_{i=1}^k \centre^k_c$ where $\centre^k_c$ is either $\centre^k$ or $\bar{\centre}^k$.

This additive structure can clearly be extended to non-binary base clusterings.
The iterative $2$-means clustering model above simply extends to an iterative $k$-means clustering.
This is exactly the orthogonal projection algorithm of \cite{Cui2007}.

\subsection{The probabilistic perspective}

By writing down a probabilistic model, we can tweak the algorithm of \cite{Cui2007} by sensible modifications to the likelihood.
Some simple avenues of enquiry include
\begin{enumerate}
\item Placing priors on the base cluster centres to justify regularised $k$-means \citep[e.g.][]{Sun2012}
\item Infer the number of clusters in each base clustering using the DP mixture model inspired algorithm of \cite{Kulis2012} (referred to as $\infty$-means below)
\item Introduce noise scaling parameters for each feature of the data to allow the model to prune out noisy features
\item Inferring the number of views through an appropriate prior (potentially IBP inspired)
\item Introducing other IBP prior terms into the optimisation to provide novel forms of regularisation\fTBD{Colorado to work out mathematics}
\end{enumerate}

\section{Improved multi-view clustering through feature selection}

In a complex and multi-faceted data set, one should expect some features to provide contradictory information, and some to just be noisy.
Thus, a successful algorithm will likely require effective feature selection.

The model of \citep{Niu2012} achieves feature selection through a two parameter IBP prior on views and features.
The `stickiness' parameter of the prior allows one to influence how different the clusterings should be a priori.
This could also be achieved via the additive means / residuals method discussed above, but restricting to certain features for each base clustering.

The work of \citep{Niu2012} provides a general framework for producing mutiple probabilistic clusterings with a Gibbs sampling implementation.
This is not entirely appropriate in the MAP setting necessitated by using a MAP algorithm for the clustering \ie $k$-means.
However, we can perform MAP inference over the entire model using a search strategy to find the optimal choice of features for each clustering view.
A good starting point for such search strategies could be \cite{Dy2004}.

\section{Next steps}

\begin{enumerate}
\item Work out the positioning of this work
\begin{enumerate}
\item Are we producing a new multiple clustering algorithm (probably not, $k$-means not the best base algorithm)
\item Are we demonstrating how to approximate richly structured nonparametric Bayesian models?
\item Are we extending the work on `nonparametric' $k$-means to multi-view `nonparametric' $k$-means?
\item Are we doing all of these things?
\end{enumerate}
\item Which models to test and in what order?
\begin{enumerate}
\item Regularised or $\infty$-means version of orthogonal projections
\item $k/\infty$-means base clustering in \cite{Niu2012}
\item $k/\infty$-means base clustering in an additive version of \cite{Niu2012}
\item Spectral clustering in \cite{Niu2012}
\item Any of the above with fast base clustering that handles missing data
\end{enumerate}
\item Any other connections to be drawn? Change focus of work?
\end{enumerate}

\section{Experiments}

\section{Discussion and conclusions}

% Bibliography

%\newpage
\small{
\bibliographystyle{assets/natbib}
%\bibliographystyle{icml2013}
\bibliography{library}
}

\end{document}
