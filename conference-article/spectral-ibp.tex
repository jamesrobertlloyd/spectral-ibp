\documentclass{article}
% Style file
\usepackage{assets/nips12submit_e,times}
% Basic packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb, bm}
\usepackage{bbm}
\usepackage{verbatim}
% More fun stuff
\usepackage{graphicx}%\graphicspath{{figures/}}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage[square,numbers]{natbib}
%\usepackage[usenames]{color}
\usepackage{mathrsfs}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,linkcolor=blue]{hyperref}
\usepackage{hypernat}
\usepackage{datetime}
\usepackage{textcomp}
\usepackage{assets/picins}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes,calc}
\tikzstyle{mybox} = [draw=white, rectangle]
\usepackage{booktabs}

% Algorithms, and some standard modifications
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\algrule}[1][.2pt]{\par\vskip.3\baselineskip\hrule height #1\par\vskip.3\baselineskip}

\title{
$k$-means, Infinite Latent Feature Models\\ and Multiple Clustering
}

\author{
James Robert Lloyd, Colorado Reed, Zoubin Ghahramani\\
Department of Engineering\\
Cambridge University\\
%\texttt{jrl44@cam.ac.uk, FIX ME}
%\And
%Coauthor \\
%Affiliation \\
%Address \\
%\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\setlength{\marginparwidth}{1.25in}

\input{assets/commenting.tex}

%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  %% Note, NA's pass through!

\input{assets/defs.tex}

\nipsfinalcopy % Uncomment for camera-ready version

\numberwithin{equation}{section}
\numberwithin{thm}{section}

% Document specific notation

\def\IBP{\boldsymbol{Z}}
\def\Weights{\boldsymbol{A}}
\def\weights{a}
\def\Data{\boldsymbol{X}}
\def\data{x}
\def\Residuals{\tilde{\boldsymbol{X}}}
\def\residuals{\tilde{x}}
\def\Graph{G}
\def\Adjacency{W}
\def\Degree{D}
\def\Laplacian{L}
\def\IBPPrior{\alpha}
\def\errors{\epsilon}
\def\bias{\boldsymbol{b}}
\def\centre{\boldsymbol{\mu}}
\def\SigData{\sigma_X}
\def\sumIBP{m}

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\begin{document}

\maketitle

\begin{abstract}
PLACEHOLDER

We demonstrate that $k$-means based multiple clustering models can be viewed as MAP inference in probabilistic models.
The probabilistic model is an approximation to a nonparametric model, and better $k$-means algorithms are developed by approximating more advanced models.
This also extends recent work on links between infinite Gaussian mixtures and $k$-means by producing a multi-view clustering algorithm.
\end{abstract}

\section{Introduction}

\PROBLEM{Placeholder}

Traditional clusterings \citep[e.g.][]{Jain2000} produce one solution.
However, data is usually more interesting and multi-faceted.

The Indian Buffet Process (IBP) is a distribution over infinite binary matrices \citep[e.g.][]{Griffiths2011}.
The IBP is often used as a prior that assigns a latent binary sequence to data points which can be interpreted as membership in multiple binary clusterings of the data.
We show that the maximum likelihood solution to a parametric version of this model is equivalent to a $k$-means algorithm for multiple clustering already in the literature \citep{Cui2007}.
By drawing a parallel with the probabilistic model and considering MAP inference we construct a regularised \citep[e.g.][]{Sun2012} and `nonparametric' modification to the $k$-means algorithm that retains efficiency.

We then extend this approximation approach to a more sophisticated Bayesian nonparametric model for multiple clustering \citep{Niu2012}.
This model automatically performs feature selection for each view, selects the number of views and the number of clusters in each view.
We again show how a limiting form of this algorithm can be approximately solved using $k$-means.
In particular, we combine feature selection methodologies of \citep{Dy2004}, with the `nonparametric' $k$-means algorithm of \cite{Kulis2012} to produce an efficient approximation to the model of \cite{Niu2012}.
\TBD{In this model, sharing of features happens through IBP `stickiness' in the two parameter version.
We could also try an additive version.
Which would be better?}

We hope that our two approximations will show how richly structured non-parametric models can in general be approximated.
One should probably compare to \eg variational inference in these models.
The scaling could potentially be a lot better though.

\section{Related work}

\TBD{To do}

\section{Simple clustering models}

\subsection{Gaussian mixture models and $k$-means}

It is well known that there is a correspondence between Gaussian mixture models and the $k$-means algorithm.
Pr\'ecising the exposition in \cite{Kulis2012}, a Gaussian mixture model assumes that data arises from the following distribution
\begin{equation}
p(x) = \sum_{c=1}^k \pi_c \, \Normal(x \given \mu_c, \Sigma_c).
\end{equation}
The $k$-means algorithm attempts to optimise the following objective
\begin{equation}
\min_{(\ell_c)} \sum_{c=1}^k \sum_{x \in \ell_c} ||x - \mu_c||^2 \quad \textrm{where} \quad \mu_c = \frac{1}{|\ell_c|}\sum_{x \in \ell_c}x.
\end{equation}
It can be shown that when $\Sigma_c = \sigma I$ and as $\sigma \to 0$, an EM algorithm for the Gaussian mixture model tends towards the $k$-means algorithm, establishing the correspondence.

\subsection{Binary linear Gaussian models and $k$-means}

We now show a correspondence between a simple probabilistic model for multiple clustering and $k$-means.
Consider the following model for an array of data
\begin{align}
\Data_{nd} \sim \Normal((\IBP\Weights)_{nd} + \bias_{d}, \SigData)
\end{align}
where $\IBP$ is a binary matrix, $\Weights$ is real valued weight matrix and $\bias$ is a bias vector. This is a variant of a model commonly used to demonstrate the Indian Buffet Process \citep[e.g.][]{Griffiths2011}. The log posterior in this model can be written as
%
\begin{align}
\log(p(\IBP)) + \log(p(\Weights)) + \log(p(\bias)) - \frac{1}{2\SigData^2}\sum_{n,d}(\Data_{nd} - (\IBP\Weights)_{nd} - \bias_d)^2.
\end{align}
%
Assuming uniform priors on the log scale\fTBD{\ie scale free priors for the weights and bias and a uniform prior on $\IBP$}, MAP inference is equivalent to minimising the following sum of squares\fPROBLEM{Not quite equivalent, this assumes a fixed feature assignments $m \neq k$}
%
\begin{align}
\sum_{n: \IBP_{nk} = 1}||\Residuals^k_{n:} - \centre^k||^2 + \sum_{n: \IBP_{nk} = 0}||\Residuals^k_{n:} - \bar{\centre}^k||^2
\end{align}
%
over $\centre^k$ and $\bar{\centre}^k$ where we have defined the residuals matrix 
\begin{align}
\Residuals^k_{nd} = \Data_{nd} - \sum_{m \neq k}\IBP_{nm}\Weights_{md}
\end{align}
and the cluster centers 
\begin{align}
\centre^k &=  \Weights_{k:} + \bias\\
\bar{\centre}^k &= \bias
\end{align}
The minimising $\centre$ are simply given by appropriate averages.
Thus, when jointly optimising over $\IBP_{:k}$ and $\centre$ we recover the $2$-means objective.

Therefore a reasonable heuristic for MAP inference in this model is to iteratively perform $2$-means clustering on the matrix of residuals.

\subsection{IBP Regularization}
Rather than assume a log-uniform prior for the binary matrix, we can introduce a form of regularised $2$-means clustering by placing an IBP prior on this matrix:
\begin{align}
p([\IBP]|\IBPPrior) &=  \frac{\IBPPrior^{K^+}}{\prod_{i=1}^{2^N-1}K_h!}\text{exp}(-\IBPPrior H_N)\prod_{k=1}^{K^+}\frac{(N-\sumIBP_k)!(\sumIBP_k - 1)!}{N!}
\end{align}
where $\sumIBP_k=\sum_n \IBP_{nk}$, $K^+ = \sum_k  \mathbbm{1}_{ \sumIBP_k > 0}$, $K_h$ is the number of features with binary representation (history) $h$\fTBD{Describe histories briefly here} described by \citet{Griffiths2011}, $H_N$ is the $Nth$ harmonic number, and the square brackets around $\IBP$ indicate that the probability of the matrix is computed in \textit{left-ordered form}. With the IBP prior the log posterior becomes:
%
\begin{align}
 \log(p(\IBP, \Weights, \bias|\data)) \propto & - \frac{1}{2\SigData^2} \sum_{n,d}(\Data_{nd} - (\IBP\Weights)_{nd} - \bias_d)^2 \notag \\
 & + K^+ \log\left(\frac{\IBPPrior}{N!}\right)- \sum_{h=1}^{2^N-1} \log\left(K_h!\right) -\IBPPrior H_N + \sum_{k=1}^{K^+}\log\left((N-\sumIBP_k)!(\sumIBP_k - 1)!\right) \notag \\ 
 & + \log(p(\Weights)) + \log(p(\bias)).
\end{align}
%
Again, assuming uniform priors on the log scale for the weights and bias terms, and iteratively maximising the posterior for a given $\IBP_{:k}, \Weights_{k:}, \bias$ \fTBD{Need to think more carefully about this bias term} (while fixing $\IBP_{:-k}, \Weights_{-k:}$ |equivalent to assuming a fixed residuals matrix) can be viewed as minimizing the following cost function \fTBD{what if $K^{+}$ changes?}
%
\begin{align}
g(\IBP_{:k}, \Weights_{k:}, \bias) = & \sum_{n: \IBP_{nk} = 1}||\Residuals^k_{n:} - \centre^k||^2 + \sum_{n: \IBP_{nk} = 0}||\Residuals^k_{n:} - \bar{\centre}^k||^2  \notag \\ 
& - 2\SigData^2 \log\left((N-\sumIBP_k)!(\sumIBP_k - 1)!\right) + 2\SigData^2 \log\left(\sum_{k^{'}=1}^{K^{+}} \mathbbm{1}_{\IBP_{:k^{'}} = \IBP_{:k}}\right)   \label{eq_IBPcostfun}.
\end{align}
%
The first line of the cost function is the SSE term discussed in the previous section while the second line is found by examining the IBP prior dependency on $\IBP_{:,k}$:
%
\begin{align}
\log(p(\IBP|\alpha)) = &  K^+ \log\left(\frac{\IBPPrior}{N!}\right)- \sum_{h=1}^{2^N-1} \log\left(K_h!\right) -\IBPPrior H_N + \sum_{k=1}^{K^+}\log\left((N-\sumIBP_k)!(\sumIBP_k - 1)!\right) \notag 
\\
= & - \sum_{h^{*}} \log\left(K_{h^{*}}!\right) - \log\left(\sum_{k^{'}=1}^{K^{+}} \mathbbm{1}_{\IBP_{:k^{'}} = \IBP_{:k}}\right)  \label{eq_strhist}
\\
& + \sum_{k^{'} \neq k}^{K^+}\log\left((N-\sumIBP_{k^{'}})!(\sumIBP_{k^{'}}- 1)!\right) +  \log\left((N-\sumIBP_k)!(\sumIBP_k - 1)!\right)
\\
& + K^+ \log\left(\frac{\IBPPrior}{N!}\right) -\IBPPrior H_N
\end{align}
%
where $h^{*}$ in Eq.\ \ref{eq_strhist} indicates that the histories are evaluated using $\IBP_{:-k}$. Removing terms that do not depend on $\IBP_{:k}$ yields
%
\begin{align}
- \log\left(\sum_{k^{'}=1}^{K^{+}} \mathbbm{1}_{\IBP_{:k^{'}} = \IBP_{:k}}\right)  +  \log\left((N-\sumIBP_k)!(\sumIBP_k - 1)!\right),
\end{align} 
%
which is the negative of the IBP term in Eq.\ \ref{eq_IBPcostfun} multiplied by $2\SigData^2$. The IBP prior effectively introduces a regularization term that penalises replicated and  balanced feature assignments (indirectly penalising similar features). 

We can minimise $g(\IBP_{:k}, \Weights_{k:}, \bias)$ via a modified 2-means clustering on the matrix of residuals by first noting the $2\SigData^2 \log\left(\sum_{k^{'}=1}^{K^{+}} \mathbbm{1}_{\IBP_{:k^{'}} = \IBP_{:k}}\right) \geq 0$ so that 
\begin{align}
g_{lb}(\IBP_{:k}, \Weights_{k:}, \bias) =&~ g(\IBP_{:k}, \Weights_{k:}, \bias) - 2\SigData^2 \log\left(\sum_{k^{'}=1}^{K^{+}} \mathbbm{1}_{\IBP_{:k^{'}} = \IBP_{:k}}\right) \\
 \leq &~ g(\IBP_{:k}, \Weights_{k:}, \bias) 
\end{align}
%
As in the $k$-means algorithm we have an assignment step in which we assign each element to a cluster and a maximization step in which we optimize the cluster locations. Given the cluster locations, we note that if we knew the optimal $\sumIBP_k$ then $\log\left((N-\sumIBP_k)!(\sumIBP_k - 1)!\right)$ is constant, and we could minimise $g_{lb}$ by assigning the $\sumIBP_k$ observations of $\Residuals^k$ to $\centre^k$ with the largest ``net distance," determined via
\begin{align}
||\Residuals^k_{n:} - \bar{\centre}^k||^2 - ||\Residuals^k_{n:} - \centre^k||^2.
\end{align}
We show this property in the Appendix. Since we do not know the optimal $\sumIBP_k$, we let $\sumIBP_k = 1,\ldots,N$, minimize $g_{lb}$ with $\sumIBP_k$ fixed, and store the resulting $g_{lb}$ value. If our goal was to minimize $g_{lb}$ then we would select the cluster assignments corresponding to the minimum $g_{lb}$ value obtained across all $m_k$ values.

Instead, we locate the cluster assignments corresponding to the minimum $g$ values using Algorithm \ref{alg1}. On a higher level, this algorithm operates by iteratively checking if the minimum lower bound across all $m_k$ is a tight bound, if it is not, then for the $m_k$ corresponding to the lower bound it finds either the next smallest lower bound or current/previous tight bound (whichever is smallest) and assigns that bound as the minimum for the given $m_k$. This process then rechecks the bounds across all $m_k$, finds the minimum and repeats the above process until the minimum bound across all $m_k$ is tight. In this case, the tight bound will be less than all other [tight] lower bounds and will therefore be the minimum. Searching for the $qth$ smallest lower bound takes exponential time in the size of $q$, however, in the worst case we will only need to perform this search up to $q=K^{+} -1$ since a maximum of $K^{+} -1$ lower bounds will not be tight. In IBP models we expect $K$ to scale  on the order of $\log(N)$ and therefore in the worst case this search technique will scale linearly with the data. (this is a quick/dirty explanation and I'll work on making it more mathematically precise, less verbose, etc)\fTBD{colorado is still working on formal running time of search aspect, though it seems that it will marginally contribute in practice}.

\begin{algorithm}                     			
\caption{Minimize $g$ via $g_{lb}$}
\label{alg1}                           
\begin{algorithmic} 
    \Require $g_{lb}$ values, distances, and corresponding assignments for each $m_k=1,\ldots,N$
    \Ensure cluster configuration to minimize $g$
        \State initialize $minheap$ to an empty priority queue  
    \For{$m_k = 1,\ldots, N$}
    \State initialize $lb\_struct_{m_k}$ to a structure with fields $\{ current\_min\_asn, min\_val, min\_type, min\_tight\_bound \}$
    \State set\_field($lb\_struct_{m_k}$, $min\_val$) $\Leftarrow$ min $g_{lb}$ assignment for $m_k$
    \State set\_field($lb\_struct_{m_k}$, $min\_val$) $\Leftarrow g_{lb}$ for $m_k$
    \State set\_field($lb\_struct_{m_k}$, $min\_type$) $\Leftarrow$ ``lower bound"
    \State add $lb\_struct_{m_k}$ to $minheap$ with $min\_val$ sort-field 
    \EndFor      
    
    \While{get\_field( get\_next($minheap$), $min\_type$) is ``lower bound" }
    	\State $current\_min\_struct \Leftarrow$ remove\_next($minheap$)
	\State $current\_min\_asn \Leftarrow$ get\_field($current\_min\_struct$, $current\_min\_asn$)
	\State $next\_largest\_asn \Leftarrow $ assignment of next largest lower bound for $current\_min\_asn$
	\State $min\_prev\_asn \Leftarrow$ get\_field($current\_min\_struct$, $min\_tight\_bound$)
	\State $min\_obs\_asn \Leftarrow \argmin_{x = \{current\_min\_asn, min\_prev\_asn\}} g(x)$ 
	\If{$g(min\_obs\_asn) \leq g_{lb}(next\_largest\_asn)$}
		\State set\_field($current\_min\_struct$, $min\_val) \Leftarrow g(min\_obs\_asn)$
		\State set\_field($current\_min\_struct$, $min\_type) \Leftarrow$ ``exact"
		\State set\_field($current\_min\_struct$, $current\_min\_asn) \Leftarrow min\_obs\_asn$
	\Else 
		\State set\_field($current\_min\_struct$, $min\_val) \Leftarrow g_{lb}(next\_largest\_asn)$
		\State set\_field($current\_min\_struct$, $min\_tight\_bound) \Leftarrow min\_obs\_asn$
		\State set\_field($current\_min\_struct$, $current\_min\_asn) \Leftarrow next\_largest\_asn$
	\EndIf

	\State add $current\_min\_struct$ to $minheap$ with $min\_val$ sort-field 
    \EndWhile
    \State $min\_struct \Leftarrow$ remove\_next($minheap$)
  \State $min\_g\_asn \Leftarrow$ get\_field($min\_struct$, $min\_asn$)
    \State \Return $min\_g\_asn$
\end{algorithmic}
\end{algorithm}

\subsection{Beyond binary clustering}

The Binary linear Gaussian model above is similar to a Gaussian mixture model, but one of the main differences is that the means of the different clusters have an additive structure \ie $\centre_c = \sum_{i=1}^k \centre^k_c$ where $\centre^k_c$ is either $\centre^k$ or $\bar{\centre}^k$.

This additive structure can clearly be extended to non-binary base clusterings.
The iterative $2$-means clustering model above simply extends to an iterative $k$-means clustering.
This is exactly the orthogonal projection algorithm of \cite{Cui2007}.

\subsection{The probabilistic perspective}

By writing down a probabilistic model, we can tweak the algorithm of \cite{Cui2007} by sensible modifications to the likelihood.
Some simple avenues of enquiry include
\begin{enumerate}
\item Placing priors on the base cluster centres to justify regularised $k$-means \citep[e.g.][]{Sun2012}
\item Infer the number of clusters in each base clustering using the DP mixture model inspired algorithm of \cite{Kulis2012} (referred to as $\infty$-means below)
\item Introduce noise scaling parameters for each feature of the data to allow the model to prune out noisy features
\item Inferring the number of views through an appropriate prior (potentially IBP inspired)
\item Introducing other IBP prior terms into the optimisation to provide novel forms of regularisation\fTBD{Colorado to work out mathematics}
\end{enumerate}

\section{Improved multi-view clustering through feature selection}

In a complex and multi-faceted data set, one should expect some features to provide contradictory information, and some to just be noisy.
Thus, a successful algorithm will likely require effective feature selection.

The model of \citep{Niu2012} achieves feature selection through a two parameter IBP prior on views and features.
The `stickiness' parameter of the prior allows one to influence how different the clusterings should be a priori.
This could also be achieved via the additive means / residuals method discussed above, but restricting to certain features for each base clustering.

The work of \citep{Niu2012} provides a general framework for producing multiple probabilistic clusterings with a Gibbs sampling implementation.
This is not entirely appropriate in the MAP setting necessitated by using a MAP algorithm for the clustering \ie $k$-means.
However, we can perform MAP inference over the entire model using a search strategy to find the optimal choice of features for each clustering view.
A good starting point for such search strategies could be \cite{Dy2004}.

\section{Next steps}

\begin{enumerate}
\item Work out the positioning of this work
\begin{enumerate}
\item Are we producing a new multiple clustering algorithm (probably not, $k$-means not the best base algorithm)
\item Are we demonstrating how to approximate richly structured nonparametric Bayesian models?
\item Are we extending the work on `nonparametric' $k$-means to multi-view `nonparametric' $k$-means?
\item Are we doing all of these things?
\end{enumerate}
\item Which models to test and in what order?
\begin{enumerate}
\item Regularised or $\infty$-means version of orthogonal projections
\item $k/\infty$-means base clustering in \cite{Niu2012}
\item $k/\infty$-means base clustering in an additive version of \cite{Niu2012}
\item Spectral clustering in \cite{Niu2012}
\item Any of the above with fast base clustering that handles missing data
\end{enumerate}
\item Any other connections to be drawn? Change focus of work?
\end{enumerate}

\section{Experiments}

\section{Discussion and conclusions}

% Bibliography

%\newpage
\small{
\bibliographystyle{assets/natbib}
%\bibliographystyle{icml2013}
\bibliography{library}
}

\appendix
\section{2-means clustering with given cluster sizes}
Given a set of data points $\mathcal{V}$, two cluster centers $\mu_1,\mu_2$ for clusters $c_1$ and $c_2$, respectively and the constraint $|c_1|=m$, we can construct the following 2-means objective:
\begin{align}
& \argmin_{c_1}\sum_{p \in c_1} d_1(p) + \sum_{p \notin c_1} d_2(p) \\
\end{align}
subject to $|c_1|=m$, where $d_i$ represents the distance from cluster center $\mu_i \in \{1,2\}$. This objective can be rewritten as
\begin{align}
 \argmin_{c_1} \sum_{p \in \mathcal{V}} d_2(p) - \sum_{p \in c_1}\left[ d_2(p) - d_1(p) \right]
\end{align}
and since the first term is constant this is equivalent to
\begin{align}
 \argmax_{c_1}\sum_{p \in c_1}\left[ d_2(p) - d_1(p) \right]
\end{align}
and by letting $d_{21} = d_2(p) - d_1(p)$ we have
\begin{align}
\label{eq_finalresc}
 \argmax_{c_1} \sum_{p \in c_1}d_{21}(p).
\end{align}
To maximize Eq.\ \ref{eq_finalresc} subject to $|c_1|=m$ we simply evaluate $d_{21}(p)$ for all $p \in \mathcal{V}$ and select the largest $m$ data points for $c_1$.


\end{document}
