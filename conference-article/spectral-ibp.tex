\documentclass{article}
% Style file
\usepackage{assets/nips12submit_e,times}
% Basic packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb, bm}
\usepackage{verbatim}
% More fun stuff
\usepackage{graphicx}%\graphicspath{{figures/}}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage[square,numbers]{natbib}
%\usepackage[usenames]{color}
\usepackage{mathrsfs}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,linkcolor=blue]{hyperref}
\usepackage{hypernat}
\usepackage{datetime}
\usepackage{textcomp}
\usepackage{assets/picins}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes,calc}
\tikzstyle{mybox} = [draw=white, rectangle]
\usepackage{booktabs}

% Algorithms, and some standard modifications
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\algrule}[1][.2pt]{\par\vskip.3\baselineskip\hrule height #1\par\vskip.3\baselineskip}

\title{
$k$-means, Infinite Latent Feature Models and Multiple Clustering
}

\author{
James Robert Lloyd, Colorado Reed, Zoubin Ghahramani\\
Department of Engineering\\
Cambridge University\\
%\texttt{jrl44@cam.ac.uk, FIX ME}
%\And
%Coauthor \\
%Affiliation \\
%Address \\
%\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\setlength{\marginparwidth}{1.25in}

\input{assets/commenting.tex}

%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  %% Note, NA's pass through!

\input{assets/defs.tex}

\nipsfinalcopy % Uncomment for camera-ready version

\numberwithin{equation}{section}
\numberwithin{thm}{section}

% Document specific notation

\def\IBP{Z}
\def\Weights{A}
\def\weights{a}
\def\Data{X}
\def\data{x}
\def\Residuals{\tilde{X}}
\def\residuals{\tilde{x}}
\def\Graph{G}
\def\Adjacency{W}
\def\Degree{D}
\def\Laplacian{L}
\def\IBPPrior{\alpha}
\def\errors{\epsilon}
\def\bias{b}
\def\centre{\beta}

\DeclareMathOperator*{\argmin}{\arg\!\min}

\begin{document}

\maketitle

\begin{abstract}
We demonstrate that $k$-means based multiple clustering models can be viewed as limiting MAP inference in probabilistic models.
The probabilistic model is an approximation to a nonparametric model, and better $k$-means algorithms are developed by approximating more advanced models.
This extends recent work on links between infinite Gaussian mixtures and $k$-means by producing a multi-view clustering algorithm.
\end{abstract}

\section{Introduction}

Traditional clusterings \citep[e.g.][]{Jain2000} produce one solution.
However, data is usually more interesting and multi-faceted.

The Indian Buffet Process (IBP) is a nonparametric Bayesian prior over binary matrices \citep[e.g.][]{Griffiths2011}.
These priors are typically used to assign a latent binary sequence to data points which can be interpreted as membership in multiple binary clusterings of the data.
We show that the maximum likelihood solution to a parametric version of this model is equivalent to a $k$-means algorithm for multiple clustering already in the literature \citep{Cui2007}.
By drawing a parallel with the probabilistic model and considering MAP inference we construct a regularised \citep[e.g.][]{Sun2012} and `nonparametric' modification to the $k$-means algorithm that retains efficiency.

We then extend this approximation approach to a more sophisticated Bayesian nonparametric model for multiple clustering \citep{Niu2012}.
This model automatically performs feature selection for each view, selects the number of views and the number of clusters in each view.
We again show how a limiting form of this algorithm can be approximately solved using $k$-means.
In particular, we combine feature selection methodologies of \citep{Dy2004}, with the `nonparametric' $k$-means algorithm of \cite{Kulis2012} to produce an efficient approximation to the model of \cite{Niu2012}.
\TBD{In this model, sharing of features happens through IBP 'stickiness' in the two parameter version.
We could also try an additive version.
Which would be better?}

We hope that our two approximations will show how richly structured non-parametric models can in general be approximated.
One should probably compare to \eg variational inference in these models.
The scaling could potentially be a lot better though.

\section{Literature review}

\TBD{Write notes here and add more literature}

\cite{Griffiths2011}
\cite{Luxburg2007}
\cite{Cui2007}
\cite{Niu2010}
\cite{Niu2011}
\cite{Niu2012}
\cite{Qi2009}
\cite{Davidson2008}
\cite{Jain2008}

\section{Binary linear Gaussian model and $k$-means}

\subsection{Note}

This correspondence is well known but is usually stated as a correspondence between EM inference in a mixture of Gaussians model and $k$-means.
This argument should probably come first, and it will make the extension to multiple (\ie not binary) clusters more clear.
However, this matrix multiplication view might be of passing interest (the notation can certainly be reduced).

\subsection{MAP preliminaries}

Start with a model of the form
\begin{equation}
\Data_{ij} = (\IBP\Weights)_{ij} + \bias_{j} + \errors_{ij}
\end{equation}
where $\IBP$ is a binary matrix, $\Weights$ is real valued weight matrix, $\bias$ is a bias vector and $\epsilon$ is a matrix of `errors'.
We will define priors as we go.

Consider a fairly general independent element prior on the errors with density
\begin{equation}
p(\errors_{ij}) \propto \exp(-|\errors_{ij}|_\psi)
\end{equation}
where $|.|_{\psi}$ is some norm with parameters $\psi$ which in turn may have a prior placed upon them.

The log posterior in this model (ignoring dependence on $\psi$ for the moment) can be written as
\begin{equation}
\log(p(\IBP)) + \log(p(\Weights)) + \log(p(\bias)) - \sum_{ij}|\Data_{ij} - (\IBP\Weights)_{ij} - \bias_j|_\psi.
\end{equation}
Assuming the prior on $\Weights$ factorises, then the relevant terms for maximising $\Weights_{k:}$ and $\bias$ keeping everything else fixed are
\begin{equation}
\log(p(\Weights_{k:})) + \log(p(\bias)) - \sum_{ij} |\Residuals^k_{ij} - \IBP_{ik}\Weights_{kj} - \bias_j|_\psi
\end{equation}
where we have defined the residuals matrix $\Residuals^k_{ij} = \sum_{m \neq k}\IBP_{im}\Weights_{mj}$.

For initial simplicity, let us assume uniform priors on the log scale (\ie the risky `scale free' priors) for the weights and bias and write their MAP estimators as $\hat{\Weights}_{k:}$ and $\hat{\bias}^k$. We then define MAP centres as
\begin{eqnarray}
\centre^k & = & \hat{\Weights}_{k:} + \hat{\bias}^k \\
\bar{\centre}^k & = & \hat{\bias}^k.
\end{eqnarray}

\subsection{Equivalence to $k$-means}

We now consider jointly maximising the posterior likelihood over $\Weights_{k:}$, $\bias$ and $\IBP_{:k}$.

Consider minimising
\begin{equation}
\sum_i||\Residuals^k_{i:} - \IBP_{ik}\hat{\Weights}_{k:} - \hat{\bias}^k||_\psi
\end{equation}
where $||.||_\psi$ is an appropriate sum of norms. We can rewrite the objective as
\begin{equation}
\sum_{i: \IBP_{ik} = 1}||\Residuals^k_{i:} - \centre^k||_\psi + \sum_{i: \IBP_{ik} = 0}||\Residuals^k_{i:} - \bar{\centre}^k||_\psi.
\end{equation}
This is exactly the $k$-means objective for two clusters.
Performing $k$-means on this problem would now be very similar to \cite{Cui2007}.

\subsection{Making it more Bayesian}

Placing Gaussian or similar priors on the weights and bias would lead to a regularised $k$-means in the MAP setting.
Relevant literature might be \cite{Sun2012}.

We could also place a Poisson prior over the number of different clusterings to infer an appropriate number of clusters.
We might also want some sort of IBP like term to decide if we should add a cluster.

\subsection{Not just binary matrices}

The binary matrix produces binary clusterings for each view.
From a clustering point of view, it is simple to see how one can extend to multiple clusterings.

\section{Approximating a more sophisticated multiple clustering model}

\citep{Niu2012} introduces a nice Bayesian nonparametric model for multi-view clustering.
Different clusterings are achieved by the IBP stickiness parameter.
We could do this.
We could also do this via an orthogonal projections type method.
This could actually win in terms of accuracy (orthogonal projections pretty good despite not performing feature selection).

% Bibliography

%\newpage
\small{
\bibliographystyle{assets/natbib}
%\bibliographystyle{icml2013}
\bibliography{library}
}

\end{document}
