\documentclass{article}
% Style file
\usepackage{assets/nips12submit_e,times}
% Basic packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb, bm}
\usepackage{verbatim}
% More fun stuff
\usepackage{graphicx}%\graphicspath{{figures/}}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage[square,numbers]{natbib}
%\usepackage[usenames]{color}
\usepackage{mathrsfs}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,linkcolor=blue]{hyperref}
\usepackage{hypernat}
\usepackage{datetime}
\usepackage{textcomp}
\usepackage{assets/picins}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes,calc}
\tikzstyle{mybox} = [draw=white, rectangle]
\usepackage{booktabs}

% Algorithms, and some standard modifications
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\algrule}[1][.2pt]{\par\vskip.3\baselineskip\hrule height #1\par\vskip.3\baselineskip}

\title{
A spectral approximation to the Indian Buffet Process
}

\author{
James Robert Lloyd, Colorado Reed, Zoubin Ghahramani\\
Department of Engineering\\
Cambridge University\\
%\texttt{jrl44@cam.ac.uk, FIX ME}
%\And
%Coauthor \\
%Affiliation \\
%Address \\
%\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\setlength{\marginparwidth}{1.25in}

\input{assets/commenting.tex}

%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  %% Note, NA's pass through!

\input{assets/defs.tex}

\nipsfinalcopy % Uncomment for camera-ready version

\numberwithin{equation}{section}
\numberwithin{thm}{section}

% Document specific notation

\def\IBP{Z}
\def\Weights{A}
\def\weights{a}
\def\Data{X}
\def\data{x}
\def\Residuals{\tilde{X}}
\def\residuals{\tilde{x}}
\def\Graph{G}
\def\Adjacency{W}
\def\Degree{D}
\def\Laplacian{L}
\def\IBPPrior{\alpha}
\def\errors{\epsilon}
\def\bias{b}
\def\centre{\beta}

\DeclareMathOperator*{\argmin}{\arg\!\min}

\begin{document}

\maketitle

\begin{abstract}
Can we use spectral methods to get a fast model based on the IBP?
Or maybe approximation schemes developed for spectral clustering can be carried over to the IBP case?
\end{abstract}

\section{Binary linear Gaussian model and $k$-means}

\subsection{MAP preliminaries}

Start with a model of the form
\begin{equation}
\Data_{ij} = (\IBP\Weights)_{ij} + \bias_{j} + \errors_{ij}
\end{equation}
where $\IBP$ is a binary matrix, $\Weights$ is real valued weight matrix, $\bias$ is a bias vector and $\epsilon$ is a matrix of `errors'.
We will define priors as we go.

Consider a fairly general independent element prior on the errors with density
\begin{equation}
p(\errors_{ij}) \propto \exp(-|\errors_{ij}|_\psi)
\end{equation}
where $|.|_{\psi}$ is some norm with parameters $\psi$ which in turn may have a prior placed upon them.

The log posterior in this model (ignoring dependence on $\psi$ for the moment) can be written as
\begin{equation}
\log(p(\IBP)) + \log(p(\Weights)) + \log(p(\bias)) - \sum_{ij}|\Data_{ij} - (\IBP\Weights)_{ij} - \bias_j|_\psi.
\end{equation}
Assuming the prior on $\Weights$ factorises, then the relevant terms for maximising $\Weights_{k:}$ and $\bias$ keeping everything else fixed are
\begin{equation}
\log(p(\Weights_{k:})) + \log(p(\bias)) - \sum_{ij} |\Residuals^k_{ij} - \IBP_{ik}\Weights_{kj} - \bias_j|_\psi
\end{equation}
where we have defined the residuals matrix $\Residuals^k_{ij} = \sum_{m \neq k}\IBP_{im}\Weights_{mj}$.

For initial simplicity, let us assume uniform priors on the log scale (\ie the risky `scale free' priors) for the weights and bias and write their MAP estimators as $\hat{\Weights}_{k:}$ and $\hat{\bias}^k$. We then define MAP centres as
\begin{eqnarray}
\centre^k & = & \hat{\Weights}_{k:} + \hat{\bias}^k \\
\bar{\centre}^k & = & \hat{\bias}^k.
\end{eqnarray}

\subsection{Equivalence to $k$-means}

We now consider jointly maximising the posterior likelihood over $\Weights_{k:}$, $\bias$ and $\IBP_{:k}$.

Consider minimising
\begin{equation}
\sum_i||\Residuals^k_{i:} - \IBP_{ik}\hat{\Weights}_{k:} - \hat{\bias}^k||_\psi
\end{equation}
where $||.||_\psi$ is an appropriate sum of norms. We can rewrite the objective as
\begin{equation}
\sum_{i: \IBP_{ik} = 1}||\Residuals^k_{i:} - \centre^k||_\psi + \sum_{i: \IBP_{ik} = 0}||\Residuals^k_{i:} - \bar{\centre}^k||_\psi.
\end{equation}
This is exactly the $k$-means objective for two clusters.
Performing $k$-means on this problem would now be very similar to \cite{Cui2007}.

\subsection{Making it more Bayesian}

Placing Gaussian or similar priors on the weights and bias would lead to a regularised $k$-means in the MAP setting.
Relevant literature might be \cite{Sun2012}.

\section{Maximum a posteriori approximated by spectral clustering}

\subsection{An approximate min cut objective}

We now consider jointly maximising the posterior likelihood over $\Weights_{k:}$, $\bias$ and $\IBP_{:k}$.
We achieve this by considering an objective which is a function of $\IBP_{:k}$ and show that it bounds the original MAP objective.

\TBD{This encoding of the MAP objective into affinities is currently quite simple. Revisiting this might allow us to do more clever things, such as altering the effective priors on the weights and bias.}

Consider minimising
\begin{equation}
\sum_i||\Residuals^k_{i:} - \IBP_{ik}\hat{\Weights}_{k:} - \hat{\bias}^k||_\psi
\end{equation}
where $||.||_\psi$ is an appropriate sum of norms. We can rewrite this as
\begin{equation}
\sum_{i: \IBP_{ik} = 1}||\Residuals^k_{i:} - \centre^k||_\psi + \sum_{i: \IBP_{ik} = 0}||\Residuals^k_{i:} - \bar{\centre}^k||_\psi.
\end{equation}
By the triangle inequality\fTBD{this is why we used a norm} we have
\begin{equation}
\sum_{ij: \IBP_{ik},\IBP_{jk} = 1}||\Residuals^k_{i:} - \Residuals^k_{j:}||_\psi \leq 2|\{\IBP_{:k} = 1\}|\sum_{i: \IBP_{ik} = 1}||\Residuals^k_{i:} - \centre^k||_\psi
\end{equation}
and by the definition of $\centre^k$ (still assuming scale free priors) we have
\begin{equation}
|\{\IBP_{:k} = 1\}|\sum_{i: \IBP_{ik} = 1}||\Residuals^k_{i:} - \centre^k||_\psi \leq \sum_{ij: \IBP_{ik},\IBP_{jk} = 1}||\Residuals^k_{i:} - \Residuals^k_{j:}||_\psi.
\end{equation}
Thus, it would not be unrelated to consider minimising
\begin{equation}
\sum_{ij: \IBP_{ik},\IBP_{jk} = 1}||\Residuals^k_{i:} - \Residuals^k_{j:}||_\psi + \sum_{ij: \IBP_{ik},\IBP_{jk} = 0}||\Residuals^k_{i:} - \Residuals^k_{j:}||_\psi
\end{equation}
which is equivalent to minimising
\begin{equation}
\sum_{ij: \IBP_{ik} = 1, \IBP_{jk} = 0}-||\Residuals^k_{i:} - \Residuals^k_{j:}||_\psi 
\end{equation}
which is a min cut problem with adjacency weights defined as $w_{ij} = -||\Residuals^k_{i:} - \Residuals^k_{j:}||_\psi$.
\TBD{Notice that all weights are negative. We could have added a constant to prevent this but this will affect the effective prior which will be saved for a separate section. Min cut is still defined in this setting (it is a finite problem) but standard exact algorithms stop working. I hope the relaxation is still valid, but we need to watch out!}
\subsection{The effective prior on $\IBP$ using spectral clustering}
Consider a generic `scaled' Laplacian of the form
\begin{equation}
\Laplacian = E^{-1/2}(\Degree - \Adjacency)E^{-1/2}
\end{equation}
for some diagonal matrix $E$. For $E = \Degree$ this is a common normalised Laplacian.
Standard spectral clustering arguments show\fTBD{write this down sometime} that the following optimisation problem
\begin{equation}
\textrm{min } f'\Laplacian f : \sum f_i = 0, ||f|| = 1
\end{equation}
is a relaxation of minimising
\begin{equation}
\frac{\sum_{ij: \IBP_{ik} = 1, \IBP_{jk} = 0}w_{ij}}{\sum_{i: \IBP_{ik} = 1}e_i} +  \frac{\sum_{ij: \IBP_{ik} = 1, \IBP_{jk} = 0}w_{ij}}{{\sum_{i: \IBP_{ik} = 0}e_i}}.
\end{equation}
For $E = I$ the denominators become the sizes of the clusters \ie ratio cut.
For $E = D$ we get volumes of clusters \ie NCut.

This means our effective prior on $\IBP$ is
\begin{equation}
\log(p(\IBP_{:k})) = \sum_{ij: \IBP_{ik} \textrm{ XOR } \IBP_{jk}}w_{ij}\Big(\frac{1}{\sum_{i: \IBP_{ik} = 1}e_i} + \frac{1}{\sum_{i: \IBP_{ik} = 0}e_i} - 1)
\end{equation}
which is data dependent!
\NA{Could we potentially modify the data likelihood?}

This problem would be `fixed' if we had a noise distribution of the form
\begin{equation}
p(\errors) \propto \sum_{ij} f(\errors_{ij})
\end{equation}
but this is beginning to look weird and I might have to be careful about signs and the partition function could probably no longer be ignored.
\NA{This is starting to look strange / unhelpful / I have low confidence in what I am writing.}

\section{Random thoughts}

Can we choose very different weights to fix things \eg log weights?
Could we also just run $k$-means on the residuals directly?
What is being spectral gaining over $k$-means?

\section{An IBP model}

Consider a simple linear Gaussian model of the form
\begin{eqnarray}
\Weights & \dist & \Normal\,(0, \sigma_\Weights^2 I) \\
\IBP & \dist & \textrm{IBP}(\IBPPrior) \\
\Data & \dist & \Normal\,(\IBP \Weights, \sigma_\Data^2 I).
\end{eqnarray}
Copying from \cite{Griffiths2011} we should be guided by terms such as
\begin{eqnarray}
p\,(\Data | \IBP, \Weights) & \propto & \exp\,(-\textrm{tr}((\Data - \IBP\Weights)\Transpose(\Data - \IBP\Weights))) \\
p\,(\Data | \IBP) & \propto & |\IBP\Transpose\IBP + \frac{\sigma_\Data^2}{\sigma_\Weights^2}I|^{D/2} \exp\,(-\textrm{tr}(\Data\Transpose(I - \IBP(\IBP\Transpose\IBP + \frac{\sigma_\Data^2}{\sigma_\Weights^2}I)^{-1}\IBP\Transpose)\Data)).
\end{eqnarray}
For simplicity, we might initially consider simpler priors on $\IBP$ and or a maximum likelihood framework.

\section{Spectral clustering notes}

For a similarity matrix $\Graph$, with adjacency matrix $\Adjacency$ and diagonal degree matrix $\Degree$, the unnormalized graph Laplacian is defined as
\begin{equation}
\Laplacian = \Degree - \Adjacency.
\end{equation}
has the following property \citep{Luxburg2007}
\begin{eqnarray}
f' \Laplacian f = \frac{1}{2}\sum w_{ij}(f_i - f_j)^2
\end{eqnarray}
and the multiplicity of the zero eigenvalue of $\Laplacian$ is equal to the number of connected components in $\Graph$.

Define a vector $f$ as follows
\begin{equation}
f_i =
\begin{cases}
\sqrt{|\bar\IBP_1|/|\IBP_1|} & \textrm{if } i \in \IBP_1\\
-\sqrt{|\IBP_1|/|\bar\IBP_1|} & \textrm{if } i \in \bar\IBP_1\\
\end{cases}.
\end{equation}
Then we have
\begin{eqnarray}
f' \Laplacian f & = & \frac{1}{2}\Bigg(\sum_{i\in \IBP_1 j \in \bar\IBP_1} w_{ij} + \sum_{i\in \bar\IBP_1 j \in \IBP_1} w_{ij}\Bigg) \Bigg(\sqrt{\frac{|\bar\IBP_1|}{|\IBP_1}|} + \sqrt{\frac{|\IBP_1|}{|\bar\IBP_1|}}\Bigg)^2 \\
& = & \textrm{Cut}(\IBP_1, \bar\IBP_1)\Bigg(\frac{|\bar\IBP_1|}{|\IBP_1|} + \frac{|\IBP_1|}{|\bar\IBP_1|} + 2\Bigg) \\
& = & \textrm{Cut}(\IBP_1, \bar\IBP_1)\Bigg(\frac{|\bar\IBP_1| + |\IBP_1|}{|\IBP_1|} + \frac{|\IBP_1| + |\bar\IBP_1|}{|\bar\IBP_1|}\Bigg) \\
& = & \textrm{RatioCut}(\IBP_1, \bar\IBP_1) \times |\Graph|.
\end{eqnarray}

Additionally, $\sum f_i = 0$, and $||f||^2 = |\Graph|$.
To complete the link one then appeals to the Rayleigh--Ritz theorem to reveal that the solution to the relaxed problem is the second eigenvalue.

\section{Relations between the two}

Suppose we are trying to estimate $\Weights$ and $\IBP$ by maximum likelihood.
In particular, consider estimating the $k$th column of $\IBP$ and the corresponding row of $\Weights$ keeping all other parameters fixed.
Our objective can be stated as trying to minimise
\begin{equation}
||\Data - \IBP_{-k}\Weights_{-k} - \IBP_{k}\Weights_{k}||
\end{equation}
where $||.||$ is some distance metric on matrices (\ie the appropriate one to make this equivalent to maximum likelihood).

\fTBD{I have implicitly assumed that $z_{ij} \in \{-1,1\}$ so that we can model bias terms etc A minor point but might be useful for some forms of identifiability}
Let $\tilde X = \Data - \IBP_{-k}\Weights_{-k}$.
For a given $\IBP_k$, the maximum likelihood estimation of $\Weights$ is equivalent to minimising
\begin{equation}
\sum_{i \in \IBP_k}|\tilde x_i - \beta_k| + \sum_{i \in \bar\IBP_k}|\tilde x_i - \bar\beta_k|
\end{equation}
over $\beta_k$ and $\bar\beta_k$.
We can now create a link to spectral clustering.

Consider the following constant
\begin{eqnarray}
C & = & \sum_{i,j}|\tilde x_i - \tilde x_j| \\
  & = & \sum_{i,j \in \IBP_k}|\tilde x_i - \tilde x_j| + \sum_{i,j \in \bar\IBP_k}|\tilde x_i - \tilde x_j| + 2\sum_{i \in \IBP_k, j \in \bar\IBP_k}|\tilde x_i - \tilde x_j|
\end{eqnarray}
and then consider maximising $\sum_{i \in \IBP_1, j \in \bar\IBP_1}|\tilde x_i - \tilde x_j|$ over $\IBP_k$.
This can be recast as minimising quantities of the form $\sum_{i \in \IBP_k, j \in \bar\IBP_k}(1 - \alpha|\tilde x_i - \tilde x_j|)$ for any $\alpha > 0$.
For small enough $\alpha$ all summands will be positive and this can be phrased as a min cut problem \ie approximate spectral clustering.

Thus, spectral clustering is approximately equivalent to minimising $\sum_{i,j \in \IBP_k}|\tilde x_i - \tilde x_j| + \sum_{i,j \in \bar\IBP_k}|\tilde x_i - \tilde x_j|$.

Let $\hat\beta_k = \argmin_\beta \sum_{i \in \IBP_k}|\tilde x_i - \beta|$. Using this definition and the triangle inequality, we get the following
\begin{equation}
|\IBP_k|\sum_{i \in \IBP_1}|\tilde x_i - \hat\beta| \leq \sum_{i,j \in \IBP_1}|\tilde x_i - \tilde x_j| \leq 2|\IBP_k|\sum_{i \in \IBP_1}|\tilde x_i - \hat\beta_k|.
\end{equation}
\ie we can show that min cut is optimising a bound on the maximum likelihood objective.

The bound is only tight when the problem is degenerate \ie this is not yet a guarantee, just a heuristic.
The tightness of these bounds could be demonstrated in a probabilistic sense by assuming the data was generated by a linear binary model.

\section{A natural iterative algorithm}

In the maximum likelihood setting this is easy, the following algorithm can be justified using the arguments above.
\begin{itemize}
\item Find a binary clustering, $\IBP_1$, using spectral clustering applied to \Data
\item Fit maximum likelihood parameters to yield $\hat\Weights_1$
\item Obtain a new clustering, $\IBP_2$, by applying spectral clustering to $\Data - \IBP_1\Weights_1$. This is a sort of iterative conditional maximisation algorithm
\item Fit maximum likelihood parameters to yield $\hat\Weights_{1:2}$
\item Obtain a new clustering, $\IBP_{3}$, by applying spectral clustering to $\Data - \IBP_{1:2}\Weights_{1:2}$. This is a sort of iterative conditional maximisation algorithm
\item et cetera\ldots with looping, split merge equivalents and other nice things developed for samplers.
\end{itemize}

\section{Can we modify the spectral clustering algorithm}

Can we include Bayesian regularisation terms into the objective?
Can we make it more similar to maximum likelihood?
How can we introduce a prior on the partitions?

Many parts of the proof that spectral clustering does what it claims are highly intertwined.
On the surface, it looks like it will be difficult to 

\section{Literature review}

\TBD{Write notes here}

\cite{Griffiths2011}
\cite{Luxburg2007}
\cite{Cui2007}
\cite{Niu2010}
\cite{Niu2011}
\cite{Niu2012}
\cite{Qi2009}
\cite{Davidson2008}

\section{$k$-means clustering as a generic preprocessing step}

I am currently using the approximate spectral clustering algorithm of \cite{Yan2009}.
Is this a good idea?
I will try to work out what is going on in this approximation.

\section{Next steps / questions}
\begin{itemize}
\item Can we modify the above argument to be more Bayesian?
\item In doing so, can we look into the min cut / spectral clustering approximation to find an appropriate graph Laplacian?
\item How different is this to orthogonal projections \citep{Cui2007}?
\item Try it? Choosing sensible parameters for the spectral clustering will not be entirely easy.
\item Can I make a guarantee about improving the marginal likelihood? Possibly not?
\end{itemize}

\section{Some relevant literature}

% Bibliography

%\newpage
\small{
\bibliographystyle{assets/natbib}
%\bibliographystyle{icml2013}
\bibliography{library}
}

\end{document}
